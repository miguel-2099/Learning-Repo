Notes for Machine Learning Foundations: Linear Algebra

• Defining Linear Algebra: 
  - Algebra: A branch of mathematics that uses mathematical statements to describe relationships between things that may vary.
  - Linear Algebra: A branch of mathematics that lets you concisely describe coordinates and interactions of planes in higher dimensions and perform operations on them.
  -  A.K.A., Linear Algebra is the study of vectors and linear functions.

• Main Building Blocks and Areas:
  - Systems of linear equations, vectors and matrices, linear transformations, determinants, and vector spaces (the study of vectors and linear functions).

• Machine Learning: (Also taught in Intro to Artificial Intelligence: 4.2.)
  - Dealing with vectors and matrices.
  - Large data sets and matrices.
  - Split data set into training and testing data set.
• Applications of Linear Algebra in ML:
  1. Data set and data files: data set is either a matrix or vector.
  2. Images and photographs.
  3. Data preparation: Dimensionality reduction or one hot encoding.
  3.1. Dimensionality Reduction:
    - Data sets are represented as matrices.
    - Use matrix factorization methods to reduce it into its constituent parts.
    - Example below:

        a11 |  a12  | a13
        a21 |  a22  | a23
    A = a31 |  a32  | a33
         .  |   .   |  . 
         .  |   .   |  .
         .  |   .   |  .
        an1 |  an2  | an3

  3.2. One Hot Encoding: 
    - Used when working with categorical data.
    - Example: Class labels for classification problems, or categorical input variables (below).
    - (It's common to encode categorical variables to make them easier to work with).

     |Color|        | Red |Green| Blue |
     | Red |        |  1  |  0  |   0  |      
     |Green| -----> |  0  |  1  |   0  |
     |Blue |        |  0  |  0  |   1  |
     |Green|        |  0  |  1  |   0  |

  4. Linear Regression:
    - The most common way of solving linear regression is via a least square optimization (matrix factorization method):
    - Example below:

             3   0 (circled by blue) 
       A =  -4   2 (circled by red)
             3  -5 (circled by green)

  5. Regularization:
    - Used to prevent the model from overfitting.
    - Overfitting: used when a model is too close a fit for the available data to the point that it does not perform well w/ any new or outside data.
    - In a nutshell, overfitting is used to encourage a model to minimize the size of coefficients while it's being fit on data.

      VALUES                                                                      
        ↑
        |        
        |         _*_    _*_   _*_    _*_
        |       */   \ /   \  /   \  /   │
        |      *└---┐* *    *     *  *┌---┘*                 ■
        |     *┌----┘*               *└----┐*          *┌----┘*
        |     *└-┐*                    *┌--┘*         *└---┐*
        |   *┌--┘*                     *└---┐*      *┌----┘*                        
        |   *└-┐*                       *┌--┘*     *└---┐* 
        |  *┌--┘*                       *└---┐*  *┌-----┘*
        |  *└-┐*                         *┌--┘*  *└-┐*
        |*■---┘*                         *└--┐ ┌----┘* 
        |                                     *
        |-------------------------------------> TIME         
                       OVERFITTED MODEL                                                  
        
      VALUES
        ↑                                                                   
        |                                                          
        |         *  *  *  *  *  *  *                                         
        |        * ⎾ -  -  -  -  -  ⏋*                            
        |       * / *  *  *  *  *  * \ *              ■                   
        |      * │ *                * | *         *  / *                          
        |     * / *                  * \ *      *  / *              
        |    * | *                    * | *     * | *              
        |   * / *                      * \ *   * / *                 
        |  * │ *                        * │| *  * │ *                 
        | * / *                          * \ * * / *               
        |* ■┘*                             *  ⏘  *      
        |                                     *
        |-------------------------------------> TIME                     
                                                                              
                    GOOD FIT / ROBUST MODEL                                         

  6. Principal Component Analysis: Method for automatically reducing the number of columns of a dataset:
    - Used in Machine Learning f]to create a projection of high-dimensional data for visualizations and training models.
    - The core of PCA methods is matrix factorization method.
    - Example:

            ⎾  3  0  4  !6  (blue colored)
        A = | -4  2  2   !2  (red colored)
            ⎿  3 -5  4  !2  (green colored)

  7. Deep Learning: Scaled up to multiple dimensions, deep learning methods work w/ vectors, matrices, and tensors of inputs and coefficients (specific subfield of Machine Learning).










































